{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vsmJpAvTreo0"
   },
   "source": [
    "\n",
    "Evaluation Metrics\n",
    "=============================\n",
    "\n",
    "Adapted from: **Pytorch_CNN_template** <https://colab.research.google.com/drive/1iDfYtWPQ4ku51Rn4-FEy4JviiyRKDekr>\n",
    "\n",
    "\n",
    "Note: the following code only calculates accuracy for now. We are hoping to add more metrics(precision, recall, f1 core) ASAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6tViFsQRHM0C"
   },
   "source": [
    "## Import Packages and Set Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "csN9l8pPHM0C",
    "outputId": "4f76a2b8-8bff-41c6-d1e6-77c5ac9f1749"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "#import pakages\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import copy\n",
    "import torch\n",
    "import pandas as pd\n",
    "from skimage import io, transform, color\n",
    "from sklearn import metrics\n",
    "import sklearn.utils.class_weight\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import utils\n",
    "from torchvision.io import read_image\n",
    "import torchvision.transforms as T\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, models\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "plt.ion()   # interactive mode\n",
    "\n",
    "# Detect if we have a GPU available\n",
    "global device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "vyu_8PW5HM0D"
   },
   "outputs": [],
   "source": [
    "#Folder holding all images, and the csv file with the labels of those images\n",
    "#a helper file will be created during the data cleaning process and saved as encoded_fitz17k.csv. this allows it to be inspected if necessary\n",
    "image_dir = \"data/finalfitz17k/\"\n",
    "image_labels_csv = \"fitzpatrick17k.csv\"\n",
    "helper_file_folder = \"\"\n",
    "\n",
    "#data to classify with (removes columns not related to image and specific data)\n",
    "#choose one of 'three cond', 'nine cond', 'all skin cond', or 'skin tone'\n",
    "#if none selected or improperly entered, will default to 'three cond'\n",
    "to_classify_on = 'three cond'\n",
    "\n",
    "#'skewed', 'balanced', or 'small' dataset:\n",
    "#NOTE: balanced dataset does not necessarily have a balanced number of each skin type across skin types\n",
    "#NOTE: balanced dataset cuts dataset from about N=16,000 to about N=3,800 (lots of downsampling, which is not ideal)\n",
    "#NOTE: 'small' dataset is a small sample of 4000 images intended for testing without the... bulk\n",
    "#if none selected or improperly entered, will default to 'skewed'\n",
    "dataset_bias = 'skewed'\n",
    "\n",
    "#can be one of 'resnet', 'alexnet', 'vgg', 'squeezenet', 'densenet', or 'inception'\n",
    "model_name = 'resnet'\n",
    "\n",
    "#Number of classes in the dataset\n",
    "#NOTE: make sure this matches the number of conditions selected for 'to_classify_on'\n",
    "num_classes = 3\n",
    "\n",
    "#Batch size for training (change depending on how much memory you have)\n",
    "batch_size = 8\n",
    "\n",
    "#Number of epochs to train for \n",
    "num_epochs = 15\n",
    "\n",
    "#Flag for feature extracting. When False, we finetune the whole model, when True we only update the reshaped layer params\n",
    "feature_extract = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iN2r827areo6"
   },
   "source": [
    "## Model Training and Validation Code\n",
    "\n",
    "\n",
    "The ``train_eval_model`` function handles the training and validation of a given model. As input, it takes a PyTorch model, two\n",
    "dataloaders for training and validation, a loss function, an optimizer, a specified number of epochs to train and validate for, and a boolean flag for when the model is an Inception model. The *is_inception* flag is used to accomodate the *Inception v3* model, as that architecture uses an auxiliary output and the overall model loss respects both the auxiliary output and the final output, as described `here <https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958>`__.\n",
    "The function trains for the specified number of epochs and after each\n",
    "epoch runs a full validation step. It also keeps track of the best\n",
    "performing model (in terms of validation accuracy), and at the end of\n",
    "training returns the best performing model. After each epoch, the\n",
    "training and validation accuracies are printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "DXacL5Dxreo6"
   },
   "outputs": [],
   "source": [
    "def train_eval_model(model_save_name, model, train_dl, val_dl, criterion, optimizer, num_epochs=25, is_inception=False):\n",
    "    \"\"\"\n",
    "      Args:\n",
    "      model: which Pytorch CNN model we are using\n",
    "      train_dl: dataloader for training \n",
    "      val_dl: dataloader for validation\n",
    "      criterion: loss function\n",
    "      optimizer: a optimizer for training and validation\n",
    "      num_epochs: how many epochs for training and validation\n",
    "      is_inception: whether or not using a Inception model\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    val_acc_history = []\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    #Iterate through every epoch\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}:'.format(epoch, num_epochs - 1))\n",
    "\n",
    "        #Model training\n",
    "        train_loss, train_corrects = train_model(model, train_dl, criterion, optimizer, is_inception)\n",
    "        epoch_loss = train_loss / len(train_dl.dataset)\n",
    "        epoch_acc = train_corrects.double() / len(train_dl.dataset)\n",
    "        print('Train Loss: {:.4f} Acc: {:.4f}'.format(epoch_loss, epoch_acc))\n",
    "\n",
    "        #Model validation\n",
    "        val_loss, val_corrects, metrics = eval_model(model, val_dl, criterion, optimizer)\n",
    "        epoch_loss = val_loss / len(val_dl.dataset)\n",
    "        epoch_acc = val_corrects.double() / len(val_dl.dataset)\n",
    "        val_acc_history.append(epoch_acc)   #Track validation accuracy history\n",
    "        if epoch_acc > best_acc:            #Track the best performing model in terms of validation accuracy\n",
    "            best_acc = epoch_acc\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        print('Val Loss: {:.4f} Acc: {:.4f}'.format(epoch_loss, epoch_acc))\n",
    "        print(metrics)\n",
    "        \n",
    "        torch.save(model_ft.state_dict(), 'model_' + str(model_save_name) + '_epoch_' + str(epoch) + '.pth')\n",
    "\n",
    "        print()\n",
    "\n",
    "    end_time = time.time()\n",
    "    running_time = end_time - start_time\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(running_time // 60, running_time % 60))\n",
    "\n",
    "    #The best performing model (in terms of validation accuracy)\n",
    "    print('Best Val Acc: {:4f}'.format(best_acc))\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    \n",
    "    torch.save(model_ft.state_dict(), 'model_' + str(model_save_name) + '_final.pth')\n",
    "\n",
    "    #Return the best model and validation accuracy history\n",
    "    return model, val_acc_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "L3ZOoPcxFY_1"
   },
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, criterion, optimizer, is_inception=False):\n",
    "    \"\"\" Helper function for train_eval_model() \"\"\"\n",
    "\n",
    "    model.train()             #Training mode\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    for inputs, labels in dataloader:\n",
    "      inputs = inputs.to(device=device, dtype=torch.float)\n",
    "      labels = labels.to(device=device, dtype=torch.float)\n",
    "      optimizer.zero_grad()\n",
    "      with torch.set_grad_enabled(True):\n",
    "          if is_inception:    #If using Inception model\n",
    "              outputs, aux_outputs = model(inputs)\n",
    "              loss1 = criterion(outputs, labels)\n",
    "              loss2 = criterion(aux_outputs, labels)\n",
    "              loss = loss1 + 0.4*loss2\n",
    "          else:               #If using other models\n",
    "              _, labels = torch.max(labels, 1)\n",
    "              outputs = model(inputs)\n",
    "              loss = criterion(outputs, labels)\n",
    "          _, preds = torch.max(outputs, 1)\n",
    "          loss.backward()\n",
    "          optimizer.step()\n",
    "          running_loss += loss.item() * inputs.size(0)\n",
    "          running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "    #Return statistics about accuracy\n",
    "    return running_loss, running_corrects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "KntQINqnGnmD"
   },
   "outputs": [],
   "source": [
    "def eval_model(model, dataloader, criterion, optimizer):\n",
    "    \"\"\" Helper function for train_eval_model() \"\"\"\n",
    "\n",
    "    model.eval()               #Validation mode\n",
    "    running_loss = 0.0         #Track statistics\n",
    "    running_corrects = 0\n",
    "    for inputs, labels in dataloader:\n",
    "      inputs = inputs.to(device=device, dtype=torch.float)\n",
    "      labels = labels.to(device=device, dtype=torch.float)\n",
    "      optimizer.zero_grad()\n",
    "      with torch.set_grad_enabled(False):\n",
    "          _, labels = torch.max(labels, 1)\n",
    "          outputs = model(inputs)\n",
    "          loss = criterion(outputs, labels)\n",
    "          _, preds = torch.max(outputs, 1)\n",
    "          running_loss += loss.item() * inputs.size(0)\n",
    "          running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "          # PRECISION, RECALL, F1\n",
    "          \n",
    "          #if on gpu, convert to be on cpu first (or sklearn metrics.classification_report cannot handle it)\n",
    "          labels = labels.to(device = 'cpu')\n",
    "          preds = preds.to(device = 'cpu')\n",
    "\n",
    "          out_metrics = metrics.classification_report(labels, preds)\n",
    "    \n",
    "    #Return statistics about accuracy\n",
    "    return running_loss, running_corrects, out_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XzYVUEHSdcLd"
   },
   "source": [
    "# Test (using the case in the template file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kUM4cInieO9-"
   },
   "source": [
    "## Initialize a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6A9KD3edreo7"
   },
   "source": [
    "Set Model Parametersâ€™ .requires_grad attribute\n",
    "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "This helper function sets the ``.requires_grad`` attribute of the\n",
    "parameters in the model to False when we are feature extracting. By\n",
    "default, when we load a pretrained model all of the parameters have\n",
    "``.requires_grad=True``, which is fine if we are training from scratch\n",
    "or finetuning. However, if we are feature extracting and only want to\n",
    "compute gradients for the newly initialized layer then we want all of\n",
    "the other parameters to not require gradients. This will make more sense\n",
    "later.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "BrnHiE7Ireo8"
   },
   "outputs": [],
   "source": [
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "my1XlPaereo-"
   },
   "outputs": [],
   "source": [
    "def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n",
    "    # Initialize these variables which will be set in this if statement. Each of these\n",
    "    #   variables is model specific.\n",
    "    model_ft = None\n",
    "    input_size = 0\n",
    "\n",
    "    if model_name == \"resnet\":\n",
    "        \"\"\" Resnet18\n",
    "        \"\"\"\n",
    "        model_ft = models.resnet18(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"alexnet\":\n",
    "        \"\"\" Alexnet\n",
    "        \"\"\"\n",
    "        model_ft = models.alexnet(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"vgg\":\n",
    "        \"\"\" VGG11_bn\n",
    "        \"\"\"\n",
    "        model_ft = models.vgg11_bn(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"squeezenet\":\n",
    "        \"\"\" Squeezenet\n",
    "        \"\"\"\n",
    "        model_ft = models.squeezenet1_0(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n",
    "        model_ft.num_classes = num_classes\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"densenet\":\n",
    "        \"\"\" Densenet\n",
    "        \"\"\"\n",
    "        model_ft = models.densenet121(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier.in_features\n",
    "        model_ft.classifier = nn.Linear(num_ftrs, num_classes) \n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"inception\":\n",
    "        \"\"\" Inception v3 \n",
    "        Be careful, expects (299,299) sized images and has auxiliary output\n",
    "        \"\"\"\n",
    "        model_ft = models.inception_v3(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        # Handle the auxilary net\n",
    "        num_ftrs = model_ft.AuxLogits.fc.in_features\n",
    "        model_ft.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        # Handle the primary net\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 299\n",
    "\n",
    "    else:\n",
    "        print(\"Invalid model name, exiting...\")\n",
    "        exit()\n",
    "    \n",
    "    return model_ft, input_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yf39c54hreo_"
   },
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "AyVQFqTJ4aqV"
   },
   "outputs": [],
   "source": [
    "#CUSTOM DATASET CLASS FOR FITZPATRICK DATASET - needed to load the data\n",
    "#normalization is hard-coded in, augmentation is added as 'transforms = ' at instantiation\n",
    "class FitzDataset(Dataset):\n",
    "  \"\"\"Fitzpatrick17k Dataset\"\"\"\n",
    "\n",
    "  def __init__(self, csv_file, img_dir, transform=None, target_transform=None):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      csv_file (string): Path to the csv file with the metadata/labels for images.\n",
    "      img_dir (string): Directory with all the images.\n",
    "      transform (callable, optional): Optional transform to be applied\n",
    "          on the image of a sample.\n",
    "      target_transform (callable, optional): Optional transform to be\n",
    "          applied on the features of the sample.\n",
    "    \"\"\"\n",
    "    self.img_labels = pd.read_csv(csv_file, usecols=list(range(1, num_classes+3)))\n",
    "    self.img_dir = img_dir\n",
    "    self.transform = transform\n",
    "    self.target_transform = target_transform\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.img_labels)\n",
    "\n",
    "  #outputs image, label tuple, both as tensors\n",
    "  def __getitem__(self, index):\n",
    "    #if torch.is_tensor(index):\n",
    "    #  index = index.tolist()\n",
    "\n",
    "    #get image\n",
    "    img_name = os.path.join(self.img_dir, (self.img_labels['md5hash'][index] + '.jpg'))\n",
    "\n",
    "    \"\"\"\n",
    "    #NORMALIZATION 1 - normalize by image to reduce possible patterns produced from a certain subset of images (which reduces generalizability)\n",
    "    image = read_image(img_name) #outputs tensor\n",
    "    mean, std = torch.mean(image), torch.std(image)\n",
    "    normalize_transform = T.Normalize(mean, std)\n",
    "    image = normalize_transform(image)\n",
    "    \"\"\"\n",
    "\n",
    "    #NORMALIZATION 2 - convert from RGB colourspace to LAB colourspace (replacing normalization)\n",
    "    image = io.imread(img_name)\n",
    "    image = color.rgb2lab(image)\n",
    "    to_tensor_transform = T.ToTensor()\n",
    "    image = to_tensor_transform(image)\n",
    "\n",
    "    #get labels\n",
    "    label = self.img_labels.iloc[index, 2:]\n",
    "    label = np.array([label], dtype=np.int8)\n",
    "    label = np.reshape(label, (num_classes))\n",
    "    label = torch.from_numpy(label)\n",
    "\n",
    "    #if transforms/augmentations are to be performed on the sample, this will trigger them\n",
    "    if self.transform:\n",
    "      image = self.transform(image)\n",
    "    if self.target_transform:\n",
    "      label = self.target_transform(label)\n",
    "    \n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "kgwkP2Wjx70C"
   },
   "outputs": [],
   "source": [
    "#CLEAN/PROCESS DATA AND INSTANTIATE DATASET\n",
    "img_labels = pd.read_csv(image_labels_csv)\n",
    "\n",
    "#Clean data: remove unncessary data\n",
    "img_labels.drop(labels=['qc', 'url', 'url_alphanum'], axis = 'columns', inplace = True) #columns we don't need\n",
    "img_labels.drop(img_labels.index[img_labels['fitzpatrick'] == -1].tolist(), axis = 'index', inplace = True) #missing skin tone value\n",
    "\n",
    "#**remove duplicates**\n",
    "duplicates = ['22f1d783dd6821defafcc915a8146c41', '6de74d3051ceafe10cf3f3e8c342bad8', 'dbee4a80595e78f281e1a0938f9857be', 'a7ef35e99387ff1227baced72467dc1f','6cd57e29acb9071a6c5e5aa23aeaf0ee',\n",
    "              '3554761709cc4906ab9db13e5e46aa25', 'ea7c258aade6d510197d02b8d5012ba5', 'dbee4a80595e78f281e1a0938f9857be','38682083d6f7539a88c17d57559dcbd6','9b82bbff48d88f3bea9d30cfd96606dc',\n",
    "              '09d46db9589ff45436cda87c4abc946b', 'bf77bafaa320f244f2331ca466b96f50', '9829c11b6a2ea0a47031a865d761a670', 'ba8e7927c71912e42cde00184b691376', '34b5d983d901815b931e28ee357ddf74',\n",
    "              'bb7eaaccb79a069d59db68a3cae983cb', 'a36d079aeee1bd073859a3af6041c4f4', '8e4fcec9d635f8e9c152a23aad631eec', '0455b31fb640b89ee7375711168f318b', '0e2a24d28767bea1a4a37d1c6a4d4f31',\n",
    "              'bf0403884214daf1e41bdb522df8c8a1', '8e8674abd53e4d087da3798f478edb8c', '5eebe4328896867cce5c841d7d15d765']\n",
    "\n",
    "for img_hash in duplicates:\n",
    "  index = img_labels.index[img_labels['md5hash'] == img_hash]\n",
    "  img_labels.drop(index, axis = 'index')\n",
    "\n",
    "# (NOT FOR USE BUT JUST IN CASE)\n",
    "#class weights to balance skin types - the *goal* of this - are defined where the loss function is defined\n",
    "#get class weights and reduce bias of a greater number of a given skin condition (used for class upweighting in loss function, but easier to calculate with unencoded data)\n",
    "#if to_classify_on == 'nine cond':\n",
    "#  class_weights = sklearn.utils.class_weight.compute_class_weight(class_weight='balanced', classes=np.unique(img_labels['nine_partition_label']), y=img_labels['nine_partition_label'].to_numpy())\n",
    "#elif to_classify_on == 'all skin cond':\n",
    "#  class_weights = sklearn.utils.class_weight.compute_class_weight(class_weight='balanced', classes=np.unique(img_labels['label']), y=img_labels['label'].to_numpy())\n",
    "#else:\n",
    "#  class_weights = sklearn.utils.class_weight.compute_class_weight(class_weight='balanced', classes=np.unique(img_labels['three_partition_label']), y=img_labels['three_partition_label'].to_numpy())\n",
    "\n",
    "#Encode data with one-hot encoding (to remove possibe hierarchy/bias) and rename label new columns to be more self-explanatory\n",
    "#ex. 'label' -> 'skin cond' for specific skin condition classification\n",
    "#col_to_encode = ['fitzpatrick', 'label', 'nine_partition_label', 'three_partition_label']\n",
    "#encoded_img_lab = pd.get_dummies(img_labels, prefix = ['fitz_skin', 'skin_cond', '9_part_cond', '3_part_cond'], columns = col_to_encode)\n",
    "\n",
    "col_to_encode = ['label', 'nine_partition_label', 'three_partition_label']\n",
    "encoded_img_lab = pd.get_dummies(img_labels, prefix = ['skin_cond', '9_part_cond', '3_part_cond'], columns = col_to_encode)\n",
    "\n",
    "#data to choose from\n",
    "#skin_tone_data = pd.concat((encoded_img_lab['md5hash'], encoded_img_lab['fitzpatrick'], encoded_img_lab.filter(regex='^fitz_skin.*', axis='columns')), axis='columns') #6 skin tones (light - dark)\n",
    "#skin_cond_data = pd.concat((encoded_img_lab['md5hash'], encoded_img_lab['fitzpatrick'], encoded_img_lab.filter(regex='^skin_cond.*', axis='columns')), axis='columns') #over 100 skin conditions to classify\n",
    "#nine_cond_data = pd.concat((encoded_img_lab['md5hash'], encoded_img_lab['fitzpatrick'], encoded_img_lab.filter(regex='^9_part_cond.*', axis='columns')), axis='columns') #9 skin condition categories to classify\n",
    "#three_cond_data = pd.concat((encoded_img_lab['md5hash'], encoded_img_lab['fitzpatrick'], encoded_img_lab.filter(regex='^3_part_cond.*', axis='columns')), axis='columns') #3 skin condition categories to classify\n",
    "\n",
    "#set to data of choice - defaults to three condition classification\n",
    "if to_classify_on == 'nine cond':\n",
    "  encoded_img_lab = pd.concat((encoded_img_lab['md5hash'], encoded_img_lab['fitzpatrick'], encoded_img_lab.filter(regex='^9_part_cond.*', axis='columns')), axis='columns') #9 skin condition categories to classify\n",
    "elif to_classify_on == 'all skin cond':\n",
    "  encoded_img_lab = skin_cond_data = pd.concat((encoded_img_lab['md5hash'], encoded_img_lab['fitzpatrick'], encoded_img_lab.filter(regex='^skin_cond.*', axis='columns')), axis='columns') #over 100 skin conditions to classify\n",
    "#elif to_classify_on == 'skin tone':\n",
    "#  encoded_img_lab = three_cond_data\n",
    "else:\n",
    "  encoded_img_lab = pd.concat((encoded_img_lab['md5hash'], encoded_img_lab['fitzpatrick'], encoded_img_lab.filter(regex='^3_part_cond.*', axis='columns')), axis='columns') #3 skin condition categories to classify\n",
    "\n",
    "\n",
    "encoded_img_lab.to_csv(os.path.join(helper_file_folder, \"encoded_fitz17k.csv\"))\n",
    "fitz_dataset = FitzDataset(csv_file = os.path.join(helper_file_folder, \"encoded_fitz17k.csv\"), \n",
    "                           img_dir = image_dir, \n",
    "                           transform = T.Compose([T.Resize(size = (400, 600)),\n",
    "                                                  #data augmentations\n",
    "                                                  T.RandomHorizontalFlip(p = 0.5),\n",
    "                                                  T.RandomVerticalFlip(p = 0.5),\n",
    "                                                  T.CenterCrop(size = (int(400*0.8) if np.random.uniform(0,1) < 0.114 else 400, int(600*0.8) if np.random.uniform(0,1) < 0.114 else 600))]))  # center crop with prob around 0.2 for both sides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "_OmIYkgfyC3K"
   },
   "outputs": [],
   "source": [
    "data_to_classify = pd.read_csv(os.path.join(helper_file_folder, \"encoded_fitz17k.csv\"), usecols=list(range(1, num_classes+3)))\n",
    "\n",
    "#GET SUBDATASETS FOR EACH SKIN CONDITION\n",
    "#indices for each skin condition\n",
    "skin_1 = data_to_classify.index[data_to_classify['fitzpatrick'] == 1].tolist()\n",
    "skin_2 = data_to_classify.index[data_to_classify['fitzpatrick'] == 2].tolist()\n",
    "skin_3 = data_to_classify.index[data_to_classify['fitzpatrick'] == 3].tolist()\n",
    "skin_4 = data_to_classify.index[data_to_classify['fitzpatrick'] == 4].tolist()\n",
    "skin_5 = data_to_classify.index[data_to_classify['fitzpatrick'] == 5].tolist()\n",
    "skin_6 = data_to_classify.index[data_to_classify['fitzpatrick'] == 6].tolist()\n",
    "\n",
    "#get train/test splits and create dataloaders for each\n",
    "#roughly a 70/20/10 train/val/test split\n",
    "if dataset_bias == 'balanced':\n",
    "  #FOR A BALANCED DATASET - should have an equal number of each Fitzpatrick skin type\n",
    "  #NOTE: not necessarily a balanced number of each skin type across skin types\n",
    "  #NOTE: cuts dataset from about N=16,000 to about N=3,800 (lots of downsampling, which is not ideal)\n",
    "\n",
    "  #skin type 6 has the fewest samples, so it will limit the size\n",
    "  train_size = int(len(skin_6) * 0.7)\n",
    "  val_size = int(len(skin_6) * 0.2)\n",
    "  test_size = int(len(skin_6) * 0.1)\n",
    "\n",
    "  #skin_6 has the fewest samples, so randomly downsample all other skin types to be the same size - should another skin tone have the fewest samples, this will need to be adapted\n",
    "  #the lists returned will be the indexes used to get the subsets of the original dataset\n",
    "  #NOTE: random produces pseudorandom results, but the lists returned should be different each time unless we control the seed\n",
    "  bal_skin_1 = random.choices(skin_1, k=len(skin_6))\n",
    "  bal_skin_2 = random.choices(skin_2, k=len(skin_6))\n",
    "  bal_skin_3 = random.choices(skin_3, k=len(skin_6))\n",
    "  bal_skin_4 = random.choices(skin_4, k=len(skin_6))\n",
    "  bal_skin_5 = random.choices(skin_5, k=len(skin_6))\n",
    "  bal_skin_6 = skin_6\n",
    "\n",
    "  random.shuffle(bal_skin_1)\n",
    "  random.shuffle(bal_skin_2)\n",
    "  random.shuffle(bal_skin_3)\n",
    "  random.shuffle(bal_skin_4)\n",
    "  random.shuffle(bal_skin_5)\n",
    "  random.shuffle(bal_skin_6)\n",
    "\n",
    "  train_skin_1 = bal_skin_1[:train_size]\n",
    "  train_skin_2 = bal_skin_2[:train_size]\n",
    "  train_skin_3 = bal_skin_3[:train_size]\n",
    "  train_skin_4 = bal_skin_4[:train_size]\n",
    "  train_skin_5 = bal_skin_5[:train_size]\n",
    "  train_skin_6 = bal_skin_6[:train_size]\n",
    "\n",
    "  val_skin_1 = bal_skin_1[train_size:train_size+val_size]\n",
    "  val_skin_2 = bal_skin_2[train_size:train_size+val_size]\n",
    "  val_skin_3 = bal_skin_3[train_size:train_size+val_size]\n",
    "  val_skin_4 = bal_skin_4[train_size:train_size+val_size]\n",
    "  val_skin_5 = bal_skin_5[train_size:train_size+val_size]\n",
    "  val_skin_6 = bal_skin_6[train_size:train_size+val_size]\n",
    "    \n",
    "  test_skin_1 = bal_skin_1[train_size+val_size:]\n",
    "  test_skin_2 = bal_skin_2[train_size+val_size:]\n",
    "  test_skin_3 = bal_skin_3[train_size+val_size:]\n",
    "  test_skin_4 = bal_skin_4[train_size+val_size:]\n",
    "  test_skin_5 = bal_skin_5[train_size+val_size:]\n",
    "  test_skin_6 = bal_skin_6[train_size+val_size:]\n",
    "\n",
    "  train_list = [*train_skin_1, *train_skin_2, *train_skin_3, *train_skin_4, *train_skin_5, *train_skin_6]\n",
    "  val_list = [*val_skin_1, *val_skin_2, *val_skin_3, *val_skin_4, *val_skin_5, *val_skin_6]\n",
    "  test_list = [*test_skin_1, *test_skin_2, *test_skin_3, *test_skin_4, *test_skin_5, *test_skin_6]\n",
    "\n",
    "  bal_train_data = torch.utils.data.Subset(fitz_dataset, train_list)\n",
    "  bal_val_data = torch.utils.data.Subset(fitz_dataset, val_list)\n",
    "  bal_test_data = torch.utils.data.Subset(fitz_dataset, test_list)\n",
    "\n",
    "  train_dl = DataLoader(bal_train_data, batch_size=64, shuffle=True)\n",
    "  val_dl = DataLoader(bal_val_data, batch_size=64, shuffle=True)\n",
    "  test_dl = DataLoader(bal_test_data, batch_size=64, shuffle=True)\n",
    "elif dataset_bias == 'small':\n",
    "  small_data = random.choices(fitz_dataset, k=4000)\n",
    "  train_size = int(len(small_data)*0.7)\n",
    "  val_size = int(len(small_data)*0.2)\n",
    "  test_size = int(len(small_data)*0.1)\n",
    "\n",
    "  small_train_data, small_val_data, small_test_data = torch.utils.data.random_split(small_data, [train_size, val_size, test_size])\n",
    "\n",
    "  train_dl = DataLoader(small_train_data, batch_size=64, shuffle=True)\n",
    "  val_dl = DataLoader(small_val_data, batch_size=64, shuffle=True)\n",
    "  test_dl = DataLoader(small_test_data, batch_size=64, shuffle=True)\n",
    "else:\n",
    "  #dataset taken as a whole is biased across skin types (more light skin type samples), so skewed dataset is the whole thing\n",
    "  train_size = int(len(fitz_dataset)*0.7)\n",
    "  val_size = int(len(fitz_dataset)*0.2)\n",
    "  test_size = int(len(fitz_dataset) - val_size - train_size)\n",
    "\n",
    "  skew_train_data, skew_val_data, skew_test_data = torch.utils.data.random_split(fitz_dataset, [train_size, val_size, test_size])\n",
    "\n",
    "  train_dl = DataLoader(skew_train_data, batch_size=64, shuffle=True)\n",
    "  val_dl = DataLoader(skew_val_data, batch_size=64, shuffle=True)\n",
    "  test_dl = DataLoader(skew_test_data, batch_size=64, shuffle=True)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "som7XPYtrepC"
   },
   "source": [
    "Create the Optimizer\n",
    "--------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wFgXUn4LrepD",
    "outputId": "2f2b9647-6f58-41ea-9b3c-b27228a4b3f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params to learn:\n",
      "\t fc.weight\n",
      "\t fc.bias\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model for this run\n",
    "# Models to choose from [resnet, alexnet, vgg, squeezenet, densenet, inception]\n",
    "model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)\n",
    "\n",
    "# Gather the parameters to be optimized/updated in this run. If we are\n",
    "#  finetuning we will be updating all parameters. However, if we are \n",
    "#  doing feature extract method, we will only update the parameters\n",
    "#  that we have just initialized, i.e. the parameters with requires_grad\n",
    "#  is True.\n",
    "params_to_update = model_ft.parameters()\n",
    "print(\"Params to learn:\")\n",
    "if feature_extract:\n",
    "    params_to_update = []\n",
    "    for name,param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            params_to_update.append(param)\n",
    "            print(\"\\t\",name)\n",
    "else:\n",
    "    for name,param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            print(\"\\t\",name)\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.SGD(params_to_update, lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class CustomCrossEntropyLoss(_WeightedLoss):\n",
    "#    def __init__(self, weight: Optional[Tensor] = None) -> None:\n",
    "#        super(CustomCrossEntropyLoss, self).__init__(weight)\n",
    "#        \n",
    "#    def custom_cross_entropy(input, target, weight=self.weight):\n",
    "# \n",
    "#\n",
    "#   def forward(self, input: Tensor, target: Tensor) -> Tensor:\n",
    "#        return F.cross_entropy(input, target, weight=self.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ny6UsF10repE"
   },
   "source": [
    "Run Training and Validation Step\n",
    "--------------------------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g8EqvIglrepE",
    "outputId": "f287c5fe-68ce-419d-bf00-4a5c4fb933a0",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/14:\n",
      "Train Loss: 0.7343 Acc: 0.7314\n",
      "Val Loss: 0.7088 Acc: 0.7311\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       2.0\n",
      "           2       0.00      0.00      0.00       0.0\n",
      "\n",
      "    accuracy                           0.00       2.0\n",
      "   macro avg       0.00      0.00      0.00       2.0\n",
      "weighted avg       0.00      0.00      0.00       2.0\n",
      "\n",
      "\n",
      "Epoch 1/14:\n",
      "Train Loss: 0.6924 Acc: 0.7415\n",
      "Val Loss: 0.6842 Acc: 0.7374\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           2       1.00      1.00      1.00         2\n",
      "\n",
      "    accuracy                           1.00         2\n",
      "   macro avg       1.00      1.00      1.00         2\n",
      "weighted avg       1.00      1.00      1.00         2\n",
      "\n",
      "\n",
      "Epoch 2/14:\n",
      "Train Loss: 0.6655 Acc: 0.7468\n",
      "Val Loss: 0.6864 Acc: 0.7395\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           2       1.00      1.00      1.00         2\n",
      "\n",
      "    accuracy                           1.00         2\n",
      "   macro avg       1.00      1.00      1.00         2\n",
      "weighted avg       1.00      1.00      1.00         2\n",
      "\n",
      "\n",
      "Epoch 4/14:\n",
      "Train Loss: 0.6622 Acc: 0.7481\n",
      "Val Loss: 0.6740 Acc: 0.7386\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         1\n",
      "           2       0.50      1.00      0.67         1\n",
      "\n",
      "    accuracy                           0.50         2\n",
      "   macro avg       0.25      0.50      0.33         2\n",
      "weighted avg       0.25      0.50      0.33         2\n",
      "\n",
      "\n",
      "Epoch 5/14:\n",
      "Train Loss: 0.6533 Acc: 0.7507\n",
      "Val Loss: 0.6769 Acc: 0.7420\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           2       1.00      1.00      1.00         2\n",
      "\n",
      "    accuracy                           1.00         2\n",
      "   macro avg       1.00      1.00      1.00         2\n",
      "weighted avg       1.00      1.00      1.00         2\n",
      "\n",
      "\n",
      "Epoch 6/14:\n",
      "Train Loss: 0.6494 Acc: 0.7541\n",
      "Val Loss: 0.6718 Acc: 0.7423\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           2       1.00      1.00      1.00         2\n",
      "\n",
      "    accuracy                           1.00         2\n",
      "   macro avg       1.00      1.00      1.00         2\n",
      "weighted avg       1.00      1.00      1.00         2\n",
      "\n",
      "\n",
      "Epoch 7/14:\n",
      "Train Loss: 0.6513 Acc: 0.7529\n",
      "Val Loss: 0.6761 Acc: 0.7392\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           2       1.00      1.00      1.00         2\n",
      "\n",
      "    accuracy                           1.00         2\n",
      "   macro avg       1.00      1.00      1.00         2\n",
      "weighted avg       1.00      1.00      1.00         2\n",
      "\n",
      "\n",
      "Epoch 8/14:\n",
      "Train Loss: 0.6490 Acc: 0.7529\n",
      "Val Loss: 0.6775 Acc: 0.7411\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           2       1.00      1.00      1.00         2\n",
      "\n",
      "    accuracy                           1.00         2\n",
      "   macro avg       1.00      1.00      1.00         2\n",
      "weighted avg       1.00      1.00      1.00         2\n",
      "\n",
      "\n",
      "Epoch 9/14:\n",
      "Train Loss: 0.6454 Acc: 0.7547\n",
      "Val Loss: 0.6760 Acc: 0.7461\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           2       1.00      1.00      1.00         2\n",
      "\n",
      "    accuracy                           1.00         2\n",
      "   macro avg       1.00      1.00      1.00         2\n",
      "weighted avg       1.00      1.00      1.00         2\n",
      "\n",
      "\n",
      "Epoch 10/14:\n",
      "Train Loss: 0.6406 Acc: 0.7556\n",
      "Val Loss: 0.6732 Acc: 0.7436\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           2       1.00      1.00      1.00         2\n",
      "\n",
      "    accuracy                           1.00         2\n",
      "   macro avg       1.00      1.00      1.00         2\n",
      "weighted avg       1.00      1.00      1.00         2\n",
      "\n",
      "\n",
      "Epoch 11/14:\n",
      "Train Loss: 0.6412 Acc: 0.7566\n",
      "Val Loss: 0.6735 Acc: 0.7417\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         1\n",
      "           2       0.50      1.00      0.67         1\n",
      "\n",
      "    accuracy                           0.50         2\n",
      "   macro avg       0.25      0.50      0.33         2\n",
      "weighted avg       0.25      0.50      0.33         2\n",
      "\n",
      "\n",
      "Epoch 12/14:\n",
      "Train Loss: 0.6390 Acc: 0.7567\n",
      "Val Loss: 0.6666 Acc: 0.7461\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         1\n",
      "           2       0.50      1.00      0.67         1\n",
      "\n",
      "    accuracy                           0.50         2\n",
      "   macro avg       0.25      0.50      0.33         2\n",
      "weighted avg       0.25      0.50      0.33         2\n",
      "\n",
      "\n",
      "Epoch 14/14:\n",
      "Val Loss: 0.6607 Acc: 0.7464\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           2       1.00      1.00      1.00         2\n",
      "\n",
      "    accuracy                           1.00         2\n",
      "   macro avg       1.00      1.00      1.00         2\n",
      "weighted avg       1.00      1.00      1.00         2\n",
      "\n",
      "\n",
      "Training complete in 464m 7s\n",
      "Best Val Acc: 0.749532\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'test_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-bc6dc466ca62>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mmodel_ft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_eval_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"skew_noupw\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_ft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_ft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_inception\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\"inception\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mnum_correct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_ft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Accuracy on the test set:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_correct\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_datasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_model' is not defined"
     ]
    }
   ],
   "source": [
    "# Send the model to GPU\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "# Setup the loss fxn\n",
    "#class_weights = sklearn.utils.class_weight.compute_class_weight(class_weight='balanced', classes=np.unique(data_to_classify['fitzpatrick']), y=data_to_classify['fitzpatrick'].to_numpy())\n",
    "#class_weights = torch.tensor(class_weights,dtype=torch.float) #needs to be tensor to be passed to torch.nn.CrossEntropyLoss\n",
    "#criterion = nn.CrossEntropyLoss(weight=class_weights).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train and evaluate\n",
    "model_ft, hist = train_eval_model(\"skew_noupw\", model_ft, train_dl, val_dl, criterion, optimizer_ft, num_epochs=num_epochs, is_inception=(model_name==\"inception\"))\n",
    "\n",
    "num_correct = test_model(model_ft, test_dl)\n",
    "print(\"Accuracy on the test set:\", str(100 * num_correct / len(test_datasets[i])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FOR A BALANCED DATASET - should have an equal number of each Fitzpatrick skin type\n",
    "#NOTE: not necessarily a balanced number of each skin type across skin types\n",
    "#NOTE: cuts dataset from about N=16,000 to about N=3,800 (lots of downsampling, which is not ideal)\n",
    "\n",
    "#skin type 6 has the fewest samples, so it will limit the size\n",
    "train_size = int(len(skin_6) * 0.7)\n",
    "val_size = int(len(skin_6) * 0.2)\n",
    "test_size = int(len(skin_6) - test_size - val_size)\n",
    "\n",
    "#skin_6 has the fewest samples, so randomly downsample all other skin types to be the same size - should another skin tone have the fewest samples, this will need to be adapted\n",
    "#the lists returned will be the indexes used to get the subsets of the original dataset\n",
    "#NOTE: random produces pseudorandom results, but the lists returned should be different each time unless we control the seed\n",
    "bal_skin_1 = random.choices(skin_1, k=len(skin_6))\n",
    "bal_skin_2 = random.choices(skin_2, k=len(skin_6))\n",
    "bal_skin_3 = random.choices(skin_3, k=len(skin_6))\n",
    "bal_skin_4 = random.choices(skin_4, k=len(skin_6))\n",
    "bal_skin_5 = random.choices(skin_5, k=len(skin_6))\n",
    "bal_skin_6 = skin_6\n",
    "\n",
    "random.shuffle(bal_skin_1)\n",
    "random.shuffle(bal_skin_2)\n",
    "random.shuffle(bal_skin_3)\n",
    "random.shuffle(bal_skin_4)\n",
    "random.shuffle(bal_skin_5)\n",
    "random.shuffle(bal_skin_6)\n",
    "\n",
    "train_skin_1 = bal_skin_1[:train_size]\n",
    "train_skin_2 = bal_skin_2[:train_size]\n",
    "train_skin_3 = bal_skin_3[:train_size]\n",
    "train_skin_4 = bal_skin_4[:train_size]\n",
    "train_skin_5 = bal_skin_5[:train_size]\n",
    "train_skin_6 = bal_skin_6[:train_size]\n",
    "\n",
    "val_skin_1 = bal_skin_1[train_size:train_size+val_size]\n",
    "val_skin_2 = bal_skin_2[train_size:train_size+val_size]\n",
    "val_skin_3 = bal_skin_3[train_size:train_size+val_size]\n",
    "val_skin_4 = bal_skin_4[train_size:train_size+val_size]\n",
    "val_skin_5 = bal_skin_5[train_size:train_size+val_size]\n",
    "val_skin_6 = bal_skin_6[train_size:train_size+val_size]\n",
    "\n",
    "test_skin_1 = bal_skin_1[train_size+val_size:]\n",
    "test_skin_2 = bal_skin_2[train_size+val_size:]\n",
    "test_skin_3 = bal_skin_3[train_size+val_size:]\n",
    "test_skin_4 = bal_skin_4[train_size+val_size:]\n",
    "test_skin_5 = bal_skin_5[train_size+val_size:]\n",
    "test_skin_6 = bal_skin_6[train_size+val_size:]\n",
    "\n",
    "train_list = [*train_skin_1, *train_skin_2, *train_skin_3, *train_skin_4, *train_skin_5, *train_skin_6]\n",
    "val_list = [*val_skin_1, *val_skin_2, *val_skin_3, *val_skin_4, *val_skin_5, *val_skin_6]\n",
    "test_list = [*test_skin_1, *test_skin_2, *test_skin_3, *test_skin_4, *test_skin_5, *test_skin_6]\n",
    "\n",
    "bal_train_data = torch.utils.data.Subset(fitz_dataset, train_list)\n",
    "bal_val_data = torch.utils.data.Subset(fitz_dataset, val_list)\n",
    "bal_test_data = torch.utils.data.Subset(fitz_dataset, test_list)\n",
    "\n",
    "train_dl2 = DataLoader(bal_train_data, batch_size=64, shuffle=True)\n",
    "val_dl2 = DataLoader(bal_val_data, batch_size=64, shuffle=True)\n",
    "test_dl2 = DataLoader(bal_test_data, batch_size=64, shuffle=True)\n",
    "\n",
    "# Initialize the model for this run\n",
    "# Models to choose from [resnet, alexnet, vgg, squeezenet, densenet, inception]\n",
    "model_ft2, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)\n",
    "\n",
    "# Gather the parameters to be optimized/updated in this run. If we are\n",
    "#  finetuning we will be updating all parameters. However, if we are \n",
    "#  doing feature extract method, we will only update the parameters\n",
    "#  that we have just initialized, i.e. the parameters with requires_grad\n",
    "#  is True.\n",
    "params_to_update = model_ft.parameters()\n",
    "print(\"Params to learn:\")\n",
    "if feature_extract:\n",
    "    params_to_update = []\n",
    "    for name,param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            params_to_update.append(param)\n",
    "            print(\"\\t\",name)\n",
    "else:\n",
    "    for name,param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            print(\"\\t\",name)\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft2 = optim.SGD(params_to_update, lr=0.001, momentum=0.9)\n",
    "\n",
    "# Send the model to GPU\n",
    "model_ft2 = model_ft2.to(device)\n",
    "\n",
    "# Setup the loss fxn\n",
    "#class_weights = sklearn.utils.class_weight.compute_class_weight(class_weight='balanced', classes=np.unique(data_to_classify['fitzpatrick']), y=data_to_classify['fitzpatrick'].to_numpy())\n",
    "class_weights = torch.tensor(class_weights,dtype=torch.float) #needs to be tensor to be passed to torch.nn.CrossEntropyLoss\n",
    "criterion2 = nn.CrossEntropyLoss(weight=class_weights).to(device)\n",
    "\n",
    "# Train and evaluate\n",
    "model_ft2, hist = train_eval_model(\"bal_upw\", model_ft2, train_dl2, val_dl2, criterion2, optimizer_ft2, num_epochs=num_epochs, is_inception=(model_name==\"inception\"))\n",
    "\n",
    "num_correct = test_model(model_ft2, test_dl2)\n",
    "print(\"Accuracy on the test set:\", str(100 * num_correct / len(test_datasets[i])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_classify = pd.read_csv(os.path.join(helper_file_folder, \"encoded_fitz17k.csv\"), usecols=list(range(1, num_classes+3)))\n",
    "\n",
    "#TEST SETS\n",
    "#by skin tone\n",
    "skin_1 = data_to_classify.index[data_to_classify['fitzpatrick'] == 1].tolist()\n",
    "skin_2 = data_to_classify.index[data_to_classify['fitzpatrick'] == 2].tolist()\n",
    "skin_3 = data_to_classify.index[data_to_classify['fitzpatrick'] == 3].tolist()\n",
    "skin_4 = data_to_classify.index[data_to_classify['fitzpatrick'] == 4].tolist()\n",
    "skin_5 = data_to_classify.index[data_to_classify['fitzpatrick'] == 5].tolist()\n",
    "skin_6 = data_to_classify.index[data_to_classify['fitzpatrick'] == 6].tolist()\n",
    "\n",
    "#random.shuffle(bal_skin_1)\n",
    "#random.shuffle(bal_skin_2)\n",
    "#random.shuffle(bal_skin_3)\n",
    "#random.shuffle(bal_skin_4)\n",
    "#random.shuffle(bal_skin_5)\n",
    "#random.shuffle(bal_skin_6)\n",
    "\n",
    "# take subsets of these to form test set\n",
    "skin1_test_data = torch.utils.data.Subset(fitz_dataset, skin_1)\n",
    "skin2_test_data = torch.utils.data.Subset(fitz_dataset, skin_2)\n",
    "skin3_test_data = torch.utils.data.Subset(fitz_dataset, skin_3)\n",
    "skin4_test_data = torch.utils.data.Subset(fitz_dataset, skin_4)\n",
    "skin5_test_data = torch.utils.data.Subset(fitz_dataset, skin_5)\n",
    "skin6_test_data = torch.utils.data.Subset(fitz_dataset, skin_6)\n",
    "\n",
    "skin1_test_dl = DataLoader(skin1_test_data, batch_size=64, shuffle=True)\n",
    "skin2_test_dl = DataLoader(skin2_test_data, batch_size=64, shuffle=True)\n",
    "skin3_test_dl = DataLoader(skin3_test_data, batch_size=64, shuffle=True)\n",
    "skin4_test_dl = DataLoader(skin4_test_data, batch_size=64, shuffle=True)\n",
    "skin5_test_dl = DataLoader(skin5_test_data, batch_size=64, shuffle=True)\n",
    "skin6_test_dl = DataLoader(skin6_test_data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, dataloader):\n",
    "    #model.eval()               #Validation mode\n",
    "    running_corrects = 0\n",
    "    for inputs, labels in dataloader:\n",
    "      inputs = inputs.to(device=device, dtype=torch.float)\n",
    "      labels = labels.to(device=device, dtype=torch.float)\n",
    "      with torch.set_grad_enabled(False):\n",
    "          _, labels = torch.max(labels, 1)\n",
    "          outputs = model(inputs)\n",
    "          _, preds = torch.max(outputs, 1)\n",
    "          running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "    #Return statistics about accuracy\n",
    "    return running_corrects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the skin type 1 test set: tensor(73.8717, device='cuda:0')\n",
      "Accuracy on the skin type 2 test set: tensor(73.5441, device='cuda:0')\n",
      "Accuracy on the skin type 3 test set: tensor(74.6977, device='cuda:0')\n",
      "Accuracy on the skin type 4 test set: tensor(77.1665, device='cuda:0')\n",
      "Accuracy on the skin type 5 test set: tensor(80.8872, device='cuda:0')\n",
      "Accuracy on the skin type 6 test set: tensor(83.7795, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "#num_correct = test_model(model_ft, test_dl)\n",
    "#print(\"Accuracy on the test set:\", str(100 * num_correct / len(skew_test_data)))\n",
    "\n",
    "test_dataloaders = [skin1_test_dl, skin2_test_dl, skin3_test_dl, skin4_test_dl, skin5_test_dl, skin6_test_dl]\n",
    "test_datasets = [skin1_test_data, skin2_test_data, skin3_test_data, skin4_test_data, skin5_test_data, skin6_test_data]\n",
    "\n",
    "for i in range(0, 6):\n",
    "    num_correct = test_model(model_ft, test_dataloaders[i])\n",
    "    print(\"Accuracy on the skin type\", str(i + 1), \"test set:\", str(100 * num_correct / len(test_datasets[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(RE)LOAD MODEL\n",
    "\n",
    "test_dataloaders = [skin1_test_dl, skin2_test_dl, skin3_test_dl, skin4_test_dl, skin5_test_dl, skin6_test_dl]\n",
    "test_datasets = [skin1_test_data, skin2_test_data, skin3_test_data, skin4_test_data, skin5_test_data, skin6_test_data]\n",
    "\n",
    "print('Balanced data: model_bal_epoch_14.pth')\n",
    "model1, input_size_ = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)\n",
    "model1.to(device)\n",
    "model1.load_state_dict(torch.load('model_bal_epoch_14.pth'))\n",
    "model1.eval()\n",
    "\n",
    "for i in range(0, 6):\n",
    "    num_correct = test_model(model1, test_dataloaders[i])\n",
    "    print(\"Accuracy on the skin type\", str(i + 1), \"test set:\", str(100 * num_correct / len(test_datasets[i])))\n",
    "\n",
    "\"\"\"\n",
    "print('Skewed data: model_epoch4.pth')\n",
    "model1, input_size_ = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)\n",
    "model1.to(device)\n",
    "model1.load_state_dict(torch.load('model_epoch4.pth'))\n",
    "model1.eval()\n",
    "\n",
    "for i in range(0, 6):\n",
    "    num_correct = test_model(model1, test_dataloaders[i])\n",
    "    print(\"Accuracy on the skin type\", str(i + 1), \"test set:\", str(100 * num_correct / len(test_datasets[i])))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Complete AI4Health EvalMet.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p36",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
